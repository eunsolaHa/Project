{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최종"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from psutil import virtual_memory\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from gc import collect\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초성/중성/종성을 결합하는 함수\n",
    "def combine_consonants(start, middle, final):\n",
    "\n",
    "    chosung = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ', '']\n",
    "    jungsung = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ', '']\n",
    "    jongsung = ['ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ', '']\n",
    "    \n",
    "    # 한글 유니코드 계산 (받침이 없는 경우)\n",
    "    no_final = 44032 + (chosung.index(start) * 21 + jungsung.index(middle)) * 28\n",
    "    if final != '':\n",
    "        # 받침이 있다면 종성 부분을 더해주기\n",
    "        yes_final = no_final + jongsung.index(final) + 1\n",
    "        # 받침이 있다면 계산된 버전을 도출\n",
    "        return chr(yes_final)\n",
    "    # 받침이 없는 경우에는 그대로 도출\n",
    "    else:\n",
    "        return chr(no_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def draw_Hangul_consonants(path_to_image, path_to_model):\n",
    "\n",
    "    # 초성, 중성, 종성 세팅\n",
    "    chosung = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ', '']\n",
    "    jungsung = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ', '']\n",
    "    jongsung = ['ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ', '']\n",
    "\n",
    "    # 모델 불러오기\n",
    "    model = tf.keras.models.load_model(path_to_model)\n",
    "    \n",
    "\n",
    "    # 이미지 변환\n",
    "    # 이미지 열기\n",
    "    image = Image.open(path_to_image)\n",
    "    # image = save_array_as_image(path_to_image)\n",
    "    \n",
    "    image = image.convert('RGB')\n",
    "    # print(image.size)\n",
    "    \n",
    "    # image = np.resize(image)\n",
    "    # 이미지 재조정\n",
    "    image = image.resize((90, 90))\n",
    "\n",
    "    # 넘파이 배열로 이미지 변환\n",
    "    image_array = np.asarray(image)\n",
    "    \n",
    "    # 픽셀값 표준화\n",
    "    image_array = [image_array.astype('float32') / 255.0]\n",
    "\n",
    "\n",
    "    # 리스트를 다시 넘파이 배열로 변환 : 이미지만 변환해서 도출\n",
    "    X = np.array(image_array)\n",
    "    \n",
    "    # 모델 예측\n",
    "    prediction = model.predict(X)\n",
    "\n",
    "    # 정답이 모여있는 리스트 만들기\n",
    "    answers = []\n",
    "\n",
    "    max_index_1 = np.argmax(prediction[0][0])\n",
    "    max_index_2 = np.argmax(prediction[1][0])\n",
    "    max_index_3 = np.argmax(prediction[2][0])\n",
    "    \n",
    "    # 하나로 묶기 / 초성, 중성, 종성값으로 변환\n",
    "    temp = [chosung[max_index_1], jungsung[max_index_2], jongsung[max_index_3]]\n",
    "    \n",
    "    # 답안지에 넣기\n",
    "    answers.append(temp)\n",
    "\n",
    "    # 둘 중 하나의 디렉토리로 지정되도록 코드 구성\n",
    "    # picked_image = Image.open(path_to_image)\n",
    "\n",
    "    # 정답과 이미지 보여주기\n",
    "    # print(f'예측 답안: {combine_consonants(*answers[0])}')\n",
    "\n",
    "    # display(picked_image)\n",
    "\n",
    "    return combine_consonants(*answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 3 syllables, 2148.8ms\n",
      "Speed: 1.3ms preprocess, 2148.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "연 흐 으\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 448x640 5 syllables, 2146.5ms\n",
      "Speed: 3.8ms preprocess, 2146.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "연 흐 양 과 점\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 448x640 5 syllables, 3960.7ms\n",
      "Speed: 1.0ms preprocess, 3960.7ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "연 희 양 과 점\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 448x640 4 syllables, 3120.8ms\n",
      "Speed: 2.3ms preprocess, 3120.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "흐 양 과 점\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "\n",
    "# 모델 불러오기\n",
    "model_YOLO = YOLO('C:/Users/TH_K/Documents/ds_study/00.Fanal_project/01.YOLOv8/data_zip/best.pt')\n",
    "model_resnet_path = 'C:/Users/TH_K/Documents/ds_study/00.Fanal_project/01.YOLOv8/ResNet_consonants_model_size90.h5'\n",
    "# 02234424.png\n",
    "\n",
    "\n",
    "# 영상 파일 경로 설정\n",
    "video_path = \"C:/Users/TH_K/Documents/ds_study/00.Fanal_project/01.YOLOv8/data_zip/20230320_200909.mp4\"\n",
    "\n",
    "# 영상 불러오기\n",
    "video = cv2.VideoCapture(video_path)\n",
    "\n",
    "# 영상 프레임 수와 FPS 가져오기\n",
    "frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# FPS가 0인 경우 1로 설정\n",
    "if fps == 0:\n",
    "    fps = 1\n",
    "\n",
    "# 1초 단위로 이미지 추출하기\n",
    "\n",
    "for i in range(0, frame_count, fps):\n",
    "    video.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "    ret, frame = video.read()\n",
    "    if ret:\n",
    "        # YOLO 모델로 객체 탐지\n",
    "        results = model_YOLO(frame, stream=True)\n",
    "\n",
    "        # 결과 좌표구하기\n",
    "        for j, r in enumerate(results):\n",
    "            boxes = r.boxes.xyxy\n",
    "            frame_with_boxes = frame.copy()\n",
    "            \n",
    "            han_ = []\n",
    "\n",
    "            # boxes가 비어 있는 경우 루프를 건너뛰기.\n",
    "            # print(boxes)\n",
    "            \n",
    "            if len(boxes) == 0:\n",
    "                continue\n",
    "            \n",
    "            boxes= sorted(boxes, key=lambda x:x[0])\n",
    "            \n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2 = [int(b) for b in box]\n",
    "                \n",
    "                cropped_image = frame_with_boxes[y1:y2, x1:x2]\n",
    "\n",
    "                alpha = 1.5  # 대비를 조절할 배율\n",
    "                beta = 30  # 밝기를 조절할 값\n",
    "                adjusted = cv2.convertScaleAbs(cropped_image, alpha=alpha, beta=beta)\n",
    "\n",
    "                kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "                emphasized = cv2.filter2D(adjusted, -1, kernel)\n",
    "                \n",
    "                # alpha1 = 1.0\n",
    "                # dst1 = np.clip((1+alpha1) * emphasized - 128 * alpha1, 0, 255).astype(np.uint8)\n",
    "                \n",
    "                dst = cv2.normalize(emphasized, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "                # plt.figure(figsize=(8,6))\n",
    "                # plt.imshow(dst)\n",
    "                # plt.show()\n",
    "                \n",
    "                cv2.imwrite('A_test.jpg', dst)\n",
    "\n",
    "                han_.append(draw_Hangul_consonants('A_test.jpg', model_resnet_path))\n",
    "\n",
    "            print(*han_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_",
   "language": "python",
   "name": "cuda_"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0807d82de1a4ea9a3a9801f2fc0ffb424618b34cdda042dcdbc011f3af8bff63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
